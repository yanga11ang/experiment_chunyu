{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   start\n",
      "end                                                     \n",
      "10000  10561 12699 10178 11002 11443 11852 12244 1020...\n",
      "10001  12225 10803 10129 10663 10155 11794 10652 1042...\n",
      "10002  10416 12222 10762 10499 10416 10416 10416 1046...\n",
      "10003  10356 12338 12307 11306 11896 10347 10334 1018...\n",
      "10004  10329 12337 10365 10331 10330 12730 12043 1035...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2139 entries, 10000 to 13402\n",
      "Data columns (total 1 columns):\n",
      "start    2139 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 33.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#数据处理\n",
    "#将终点相同的cell合并，一列终点cell，一列起点cell的字符串集合\n",
    "#数据有问题，需要先将 ' '换成',', 用txt 替换可以，\n",
    "#给数据加列名 ，user，start，end，time1，time2\n",
    "data = pd.read_csv(r'..\\data\\od.csv',usecols=['start','end']) \n",
    "data['start'] = data['start'].astype(str)\n",
    "data['end'] = data['end'].astype(str)\n",
    "data['start'] = data['start'].apply(lambda x:' '+ str(x))\n",
    "data = data[['start','end']]\n",
    "data = data.groupby(by='end').sum()\n",
    "data['start'] = data['start'].apply(lambda x: x[1:])\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "data_samples = data['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 2.906s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20 # 显示主题的前n_top_words个cell\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer() \n",
    "# tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "#                                 max_features=n_features,\n",
    "#                                 stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.940s.\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:,12348,11827,12252,11830,11831,12240,11829,12362,12244,11889,11822,12440,11835,12421,12242,11856,11890,11891,11804,12342\n",
      "\n",
      "Topic #1:,13206,13046,12497,12507,12525,13210,13402,12761,12508,12783,13383,13379,12757,12745,13387,13392,13130,12760,13103,12753\n",
      "\n",
      "Topic #2:,11957,12416,11971,11815,12214,11953,12210,12135,11961,11810,12434,11809,11787,12391,12428,12326,12127,11940,12417,12316\n",
      "\n",
      "Topic #3:,10533,13024,10449,10064,10428,10412,10466,10431,12372,10467,13354,10768,13362,13022,10089,10437,10448,13029,10018,13017\n",
      "\n",
      "Topic #4:,12231,12237,12229,12233,12234,10085,12225,12224,12230,10067,12206,12316,12254,12305,11789,10086,10804,10092,11796,10658\n",
      "\n",
      "Topic #5:,11713,11466,12334,12097,12070,12084,12079,11761,12439,11899,11772,11843,11467,11411,11408,12115,11771,11329,12094,11334\n",
      "\n",
      "Topic #6:,12501,12686,10107,12681,12699,10293,12691,10085,10098,10297,10175,13272,10181,10011,12640,12646,10193,10093,10686,10179\n",
      "\n",
      "Topic #7:,12597,12582,13278,10964,11352,12727,11340,12732,11423,10906,12723,11282,11252,10986,12724,10358,12581,10985,10970,10987\n",
      "\n",
      "Topic #8:,10109,12629,10234,12632,10227,13251,10271,10104,10230,12633,10238,12449,13256,10245,13360,12623,12631,13245,12634,12630\n",
      "\n",
      "Topic #9:,13278,12723,12679,10285,10286,10354,12718,10340,13274,10013,10010,10188,10269,10352,12695,13280,10184,10740,13281,10311\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    f = open(r'../result/lad.csv','w') \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d:,\" % topic_idx\n",
    "        message += \",\".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        message +=\"\\n\"\n",
    "        print(message)\n",
    "        f.write(message)\n",
    "    f.close()\n",
    "    print()\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
